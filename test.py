"""
PROJET DATA SCIENCE - ANALYSE DES BANQUES COOPÃ‰RATIVES EUROPÃ‰ENNES
Analyse de l'Ã©volution du business model avant/aprÃ¨s la crise financiÃ¨re 2008
PÃ©riode: 2005-2015 | PrÃ©-crise: 2005-2010 | Post-crise: 2011-2015
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import warnings
warnings.filterwarnings('ignore')

# Configuration des graphiques
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

# ============================================================================
# PHASE 1: CHARGEMENT ET EXPLORATION DES DONNÃ‰ES
# ============================================================================

print("="*80)
print("PHASE 1: CHARGEMENT DES DONNÃ‰ES")
print("="*80)

# Charger les donnÃ©es
df = pd.read_csv('Theme4_coop_zoom_data.xlsx - coop_zoom_data.csv')

# Supprimer la colonne inutile si elle existe
if 'Unnamed: 10' in df.columns:
    df = df.drop(columns=['Unnamed: 10'])

# Colonnes financiÃ¨res Ã  convertir
num_cols = ['ass_total', 'ass_trade', 'inc_trade', 'in_roa', 'rt_rwa', 'in_roe', 'in_trade']

# Remplacer les virgules par des points et convertir en float
for col in num_cols:
    df[col] = pd.to_numeric(df[col].astype(str).str.replace(',', '.'), errors='coerce')

print(f"\nðŸ“Š Dimensions du dataset: {df.shape}")
print(f"   - Observations: {df.shape[0]:,}")
print(f"   - Variables: {df.shape[1]}")

print("\nðŸ“‹ PremiÃ¨res lignes:")
print(df.head())

print("\nðŸ“… PÃ©riode temporelle:")
print(f"   - AnnÃ©es: {df['year'].min()} Ã  {df['year'].max()}")
print(f"   - Distribution par annÃ©e:")
print(df['year'].value_counts().sort_index())

print("\nðŸŒ Pays couverts:")
print(df['country_code'].value_counts())

print("\nðŸ¦ Nombre de banques uniques:", df['institution_name'].nunique())

# VÃ©rifier les valeurs manquantes
print("\nâŒ Valeurs manquantes (%):")
missing = (df.isnull().sum() / len(df) * 100).sort_values(ascending=False)
print(missing[missing > 0].head(10))

# ============================================================================
# PHASE 2: PRÃ‰PARATION DES DONNÃ‰ES
# ============================================================================

print("\n" + "="*80)
print("PHASE 2: PRÃ‰PARATION DES DONNÃ‰ES")
print("="*80)

# CrÃ©er la variable pÃ©riode
df['periode'] = df['year'].apply(lambda x: 'Pre-crise' if x <= 2010 else 'Post-crise')

print("\nðŸ“Š Distribution par pÃ©riode:")
print(df['periode'].value_counts())

# SÃ©lectionner les variables clÃ©s pour l'analyse
key_vars = [
    'ass_total', 'ass_trade', 'inc_trade', 'in_roa', 'rt_rwa', 'in_roe', 'in_trade'
]

# Variables disponibles dans ton dataset (Ã  adapter si besoin)
available_vars = [col for col in key_vars if col in df.columns]

print(f"\nâœ… Variables clÃ©s disponibles: {len(available_vars)}/{len(key_vars)}")
print(available_vars)

# CrÃ©er le dataset pour l'analyse (retirer les NaN)
df_clean = df[['institution_name', 'year', 'country_code', 'periode'] + available_vars].copy()
df_clean = df_clean.dropna(subset=available_vars)

print(f"\nðŸ§¹ AprÃ¨s nettoyage: {df_clean.shape[0]:,} observations")

# ============================================================================
# PHASE 3: ANALYSE DESCRIPTIVE
# ============================================================================

print("\n" + "="*80)
print("PHASE 3: ANALYSE DESCRIPTIVE")
print("="*80)

# Statistiques descriptives par pÃ©riode
print("\nðŸ“Š STATISTIQUES DESCRIPTIVES PAR PÃ‰RIODE\n")

for var in available_vars:
    print(f"\n{'='*60}")
    print(f"Variable: {var}")
    print(f"{'='*60}")
    
    stats_by_period = df_clean.groupby('periode')[var].describe()
    print(stats_by_period)

# CrÃ©er des visualisations comparatives
fig, axes = plt.subplots(2, 3, figsize=(18, 12))
fig.suptitle('Distribution des variables clÃ©s: PrÃ©-crise vs Post-crise', 
             fontsize=16, fontweight='bold')

for idx, var in enumerate(available_vars[:6]):  # 6 premiÃ¨res variables
    row = idx // 3
    col = idx % 3
    ax = axes[row, col]
    
    # Boxplot comparatif
    df_clean.boxplot(column=var, by='periode', ax=ax)
    ax.set_title(f'{var}')
    ax.set_xlabel('PÃ©riode')
    plt.sca(ax)
    plt.xticks(rotation=0)

plt.tight_layout()
plt.savefig('01_distribution_variables.png', dpi=300, bbox_inches='tight')
print("\nâœ… Graphique sauvegardÃ©: 01_distribution_variables.png")

# ============================================================================
# PHASE 4: TESTS STATISTIQUES (MÃ©thode 1)
# ============================================================================

print("\n" + "="*80)
print("PHASE 4: TESTS STATISTIQUES COMPARATIFS")
print("="*80)

print("\nðŸ”¬ Test t de Student: Comparaison PrÃ©-crise vs Post-crise\n")

results_tests = []

for var in available_vars:
    # SÃ©parer les donnÃ©es
    pre_crise = df_clean[df_clean['periode'] == 'Pre-crise'][var].dropna()
    post_crise = df_clean[df_clean['periode'] == 'Post-crise'][var].dropna()
    
    # Test t de Student
    t_stat, p_value = stats.ttest_ind(pre_crise, post_crise)
    
    # Cohen's d (mesure de l'effet)
    cohens_d = (pre_crise.mean() - post_crise.mean()) / np.sqrt(
        ((len(pre_crise)-1) * pre_crise.std()**2 + (len(post_crise)-1) * post_crise.std()**2) / 
        (len(pre_crise) + len(post_crise) - 2)
    )
    
    # InterprÃ©tation
    significatif = "âœ… OUI" if p_value < 0.05 else "âŒ NON"
    
    results_tests.append({
        'Variable': var,
        'Moyenne PrÃ©-crise': pre_crise.mean(),
        'Moyenne Post-crise': post_crise.mean(),
        'DiffÃ©rence (%)': ((post_crise.mean() - pre_crise.mean()) / pre_crise.mean() * 100),
        't-statistic': t_stat,
        'p-value': p_value,
        "Cohen's d": cohens_d,
        'Significatif (p<0.05)': significatif
    })
    
    print(f"\n{'='*60}")
    print(f"Variable: {var}")
    print(f"{'='*60}")
    print(f"Moyenne PrÃ©-crise:  {pre_crise.mean():.6f}")
    print(f"Moyenne Post-crise: {post_crise.mean():.6f}")
    print(f"DiffÃ©rence (%):     {((post_crise.mean() - pre_crise.mean()) / pre_crise.mean() * 100):.2f}%")
    print(f"t-statistic:        {t_stat:.4f}")
    print(f"p-value:            {p_value:.6f}")
    print(f"Cohen's d:          {cohens_d:.4f}")
    print(f"Significatif:       {significatif}")

# Sauvegarder les rÃ©sultats
df_results = pd.DataFrame(results_tests)
df_results.to_csv('02_tests_statistiques.csv', index=False)
print("\nâœ… RÃ©sultats sauvegardÃ©s: 02_tests_statistiques.csv")

# ============================================================================
# PHASE 5: CLUSTERING (MÃ©thode 2)
# ============================================================================

print("\n" + "="*80)
print("PHASE 5: CLUSTERING K-MEANS")
print("="*80)

# PrÃ©parer les donnÃ©es pour le clustering
X = df_clean[available_vars].dropna()
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# DÃ©terminer le nombre optimal de clusters (mÃ©thode du coude)
inertias = []
K_range = range(2, 11)

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(X_scaled)
    inertias.append(kmeans.inertia_)

# Visualiser la mÃ©thode du coude
plt.figure(figsize=(10, 6))
plt.plot(K_range, inertias, 'bo-', linewidth=2, markersize=8)
plt.xlabel('Nombre de clusters (k)', fontsize=12)
plt.ylabel('Inertie', fontsize=12)
plt.title('MÃ©thode du coude - DÃ©termination du nombre optimal de clusters', 
          fontsize=14, fontweight='bold')
plt.grid(True, alpha=0.3)
plt.savefig('03_elbow_method.png', dpi=300, bbox_inches='tight')
print("\nâœ… Graphique sauvegardÃ©: 03_elbow_method.png")

# Clustering avec k=4 (Ã  ajuster selon le graphique)
n_clusters = 4
kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
df_clean_for_clustering = df_clean[available_vars].dropna()
clusters = kmeans.fit_predict(X_scaled)
df_clean.loc[df_clean_for_clustering.index, 'cluster'] = clusters

print(f"\nðŸ“Š Distribution des clusters:")
print(df_clean['cluster'].value_counts().sort_index())

# Analyser les clusters par pÃ©riode
print("\nðŸ“Š Distribution des clusters par pÃ©riode:")
cluster_period = pd.crosstab(df_clean['cluster'], df_clean['periode'], normalize='columns') * 100
print(cluster_period)

# CaractÃ©riser les clusters
print("\nðŸ“Š CARACTÃ‰RISATION DES CLUSTERS (moyennes):\n")
cluster_profiles = df_clean.groupby('cluster')[available_vars].mean()
print(cluster_profiles)

cluster_profiles.to_csv('04_cluster_profiles.csv')
print("\nâœ… Profils des clusters sauvegardÃ©s: 04_cluster_profiles.csv")

# Visualisation PCA avec clusters
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

plt.figure(figsize=(12, 8))
scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], 
                     c=clusters, cmap='viridis', 
                     s=50, alpha=0.6, edgecolors='black', linewidth=0.5)
plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)', fontsize=12)
plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)', fontsize=12)
plt.title('Visualisation des clusters (PCA)', fontsize=14, fontweight='bold')
plt.colorbar(scatter, label='Cluster')
plt.grid(True, alpha=0.3)
plt.savefig('05_clusters_pca.png', dpi=300, bbox_inches='tight')
print("\nâœ… Graphique sauvegardÃ©: 05_clusters_pca.png")

# ============================================================================
# PHASE 6: ANALYSE PAR PAYS
# ============================================================================

print("\n" + "="*80)
print("PHASE 6: ANALYSE PAR PAYS")
print("="*80)

# Comparaison par pays
print("\nðŸŒ Moyenne des variables par pays et pÃ©riode:\n")

for var in available_vars[:3]:  # 3 premiÃ¨res variables
    print(f"\n{'='*60}")
    print(f"Variable: {var}")
    print(f"{'='*60}")
    pivot = df_clean.pivot_table(values=var, 
                                  index='country_code', 
                                  columns='periode', 
                                  aggfunc='mean')
    pivot['Variation (%)'] = ((pivot['Post-crise'] - pivot['Pre-crise']) / 
                               pivot['Pre-crise'] * 100)
    print(pivot)

# ============================================================================
# RÃ‰SUMÃ‰ FINAL
# ============================================================================

print("\n" + "="*80)
print("RÃ‰SUMÃ‰ DE L'ANALYSE")
print("="*80)

print("""
âœ… FICHIERS GÃ‰NÃ‰RÃ‰S:
   1. 01_distribution_variables.png - Distributions comparatives
   2. 02_tests_statistiques.csv - RÃ©sultats des tests t
   3. 03_elbow_method.png - MÃ©thode du coude
   4. 04_cluster_profiles.csv - Profils des clusters
   5. 05_clusters_pca.png - Visualisation PCA

ðŸ“Š RÃ‰SULTATS CLÃ‰S Ã€ INTERPRÃ‰TER:
   - Quelles variables ont significativement changÃ© ?
   - Combien de profils de banques identifiÃ©s ?
   - Quels pays ont Ã©tÃ© les plus affectÃ©s ?
   - Les banques sont-elles devenues plus prudentes ?

ðŸ“ PROCHAINES Ã‰TAPES:
   1. InterprÃ©ter les rÃ©sultats
   2. RÃ©diger le rapport (15 pages max)
   3. CrÃ©er l'application Streamlit
   4. PrÃ©parer la soutenance (8 min)
""")

print("\n" + "="*80)
print("ANALYSE TERMINÃ‰E !")
print("="*80)
